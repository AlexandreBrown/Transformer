{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input/Output Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/embedding_softmax.png\" height=\"150px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoke Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 128\n",
      "Sequence length: 20\n",
      "Embedding dimension: 512\n",
      "Input shape: torch.Size([128, 20])\n",
      "Embedding output shape: torch.Size([128, 20, 512])\n"
     ]
    }
   ],
   "source": [
    "tokens_dictionary_size = 1000\n",
    "d_model = 512\n",
    "embedding = nn.Embedding(num_embeddings=tokens_dictionary_size, embedding_dim=d_model)\n",
    "\n",
    "batch_size = 128\n",
    "sequence_length = 20\n",
    "\n",
    "input = torch.randint(low=0, high=tokens_dictionary_size, size=(batch_size, sequence_length))\n",
    "\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Sequence length: {sequence_length}\")\n",
    "print(f\"Embedding dimension: {d_model}\")\n",
    "\n",
    "print(f\"Input shape: {input.shape}\")\n",
    "\n",
    "print(f\"Embedding output shape: {embedding(input).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/positional_encoding.png\" height=\"150px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$pos \\in [0,seq_length - 1]$  \n",
    "$i \\in [0, d_{model - 1}]$ (i is the dimensions of the embedding)  \n",
    "Note that my implementation uses logarithmic for numerical stability purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_seq_length: int, d_model: int):\n",
    "        super().__init__()\n",
    "        self.positional_encoding_matrix = self.compute_positional_encoding_matrix(max_seq_length=max_seq_length, d_model=d_model)\n",
    "    \n",
    "    def compute_positional_encoding_matrix(self, max_seq_length: int, d_model: int) -> torch.Tensor:\n",
    "        \"\"\"Computes the positional encoding matrix of shape (max_seq_length, d_model).\n",
    "\n",
    "        Args:\n",
    "            max_seq_length (int): Maximum sequence length\n",
    "            d_model (int): Embedding dimension\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Positional encoding matrix of shape (1, max_seq_length, d_model)\n",
    "        \"\"\"\n",
    "        positions = torch.arange(start=0, end=max_seq_length).unsqueeze(1)\n",
    "        \n",
    "        even_dimensions = torch.arange(start=0, end=d_model, step=2)\n",
    "        division_term = torch.exp(even_dimensions * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        positional_encoding_matrix = torch.zeros(1, max_seq_length, d_model)\n",
    "        positional_encoding_matrix[0, :, 0::2] = torch.sin(positions * division_term)\n",
    "        positional_encoding_matrix[0, :, 1::2] = torch.cos(positions * division_term)\n",
    "        \n",
    "        return positional_encoding_matrix\n",
    "    \n",
    "    def forward(self, embedding: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of the PositionalEncoding module.\n",
    "\n",
    "        Args:\n",
    "            embedding (torch.Tensor): Embedding tensor of shape (batch_size, sequence_length, d_model)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Positional encoding tensor of shape (batch_size, sequence_length, d_model)\n",
    "        \"\"\"\n",
    "        return embedding + self.positional_encoding_matrix[:, :embedding.shape[1], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoke Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding shape: torch.Size([128, 50, 512])\n",
      "positional_encoding.shape: torch.Size([128, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "sequence_length = 50\n",
    "\n",
    "embedding = torch.randn(size=(batch_size, sequence_length, d_model))\n",
    "print(f\"embedding shape: {embedding.shape}\")\n",
    "\n",
    "max_seq_length = 1000\n",
    "d_model = 512\n",
    "\n",
    "positional_encoding = PositionalEncoding(max_seq_length=max_seq_length, d_model=d_model)\n",
    "print(f\"positional_encoding.shape: {positional_encoding.forward(embedding).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaled Dot-Product Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/scaled_dot_product_attention.png\" height=\"300px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def forward(self, queries: torch.Tensor, keys: torch.Tensor, values: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of the scaled dot product attention.\n",
    "\n",
    "        Args:\n",
    "            queries (torch.Tensor): Queries of shape (batch_size, sequence_length, d_k)\n",
    "            keys (torch.Tensor): Keys of shape (batch_size, sequence_length, d_k)\n",
    "            values (torch.Tensor): Values of shape (batch_size, sequence_length, d_v)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Outputs of the scaled dot product attention of shape (batch_size, sequence_length, d_v)\n",
    "        \"\"\"\n",
    "        d_k = queries.shape[-1]\n",
    "        dot_product = torch.bmm(queries, keys.transpose(1, 2))\n",
    "        scaled_dot_product = dot_product / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "        queries_keys_similarity_probabilities = F.softmax(scaled_dot_product, dim=1)\n",
    "        return torch.bmm(queries_keys_similarity_probabilities, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoke Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queries.shape: torch.Size([2, 20, 4])\n",
      "keys.shape: torch.Size([2, 20, 4])\n",
      "values.shape: torch.Size([2, 20, 5])\n",
      "scaled_dot_product_attention shape : torch.Size([2, 20, 5])\n"
     ]
    }
   ],
   "source": [
    "scaled_dot_product_attention = ScaledDotProductAttention()\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "sequence_length = 20\n",
    "\n",
    "d_k = 4\n",
    "\n",
    "d_v = 5\n",
    "\n",
    "queries = torch.randn(batch_size, sequence_length, d_k)\n",
    "\n",
    "keys = torch.randn(batch_size, sequence_length, d_k)\n",
    "\n",
    "values = torch.randn(batch_size, sequence_length, d_v)\n",
    "\n",
    "print(f\"queries.shape: {queries.shape}\")\n",
    "print(f\"keys.shape: {keys.shape}\")\n",
    "print(f\"values.shape: {values.shape}\")\n",
    "\n",
    "print(f\"scaled_dot_product_attention shape : {scaled_dot_product_attention(queries, keys, values).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/multi_head_attention.png\" height=\"300px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads: int, d_k: int, d_v: int, d_model: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        assert d_k == d_model // num_heads, \"d_k must be equal to d_model // num_heads\"\n",
    "        assert d_v == d_model // num_heads, \"d_v must be equal to d_model // num_heads\"\n",
    "        \n",
    "        self.queries_projections = nn.ModuleList([nn.Linear(in_features=d_model, out_features=d_k, bias=False) for _ in range(num_heads)])\n",
    "        self.keys_projections = nn.ModuleList([nn.Linear(in_features=d_model, out_features=d_k, bias=False) for _ in range(num_heads)])\n",
    "        self.values_projections = nn.ModuleList([nn.Linear(in_features=d_model, out_features=d_v, bias=False) for _ in range(num_heads)])\n",
    "        self.attentions = nn.ModuleList([ScaledDotProductAttention() for _ in range(num_heads)])\n",
    "        self.multi_head_linear = nn.Linear(in_features=num_heads*d_v, out_features=d_model)\n",
    "    \n",
    "    def forward(self, queries: torch.Tensor, keys: torch.Tensor, values: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of the multi-head attention.\n",
    "\n",
    "        Args:\n",
    "            queries (torch.Tensor): Queries of shape (batch_size, sequence_length, d_model)\n",
    "            keys (torch.Tensor): Keys of shape (batch_size, sequence_length, d_model)\n",
    "            values (torch.Tensor): Values of shape (batch_size, sequence_length, d_model)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Outputs of the multi-head attention of shape (batch_size, sequence_length, d_model)\n",
    "        \"\"\"\n",
    "\n",
    "        heads = [attention(self.queries_projections[i](queries), self.keys_projections[i](keys), self.values_projections[i](values)) for i, attention in enumerate(self.attentions)]\n",
    "        \n",
    "        heads_concatenated = torch.cat(heads, dim=2)\n",
    "        \n",
    "        return self.multi_head_linear(heads_concatenated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoke Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi head attention output shape: torch.Size([2, 20, 512])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "num_heads = 8\n",
    "sequence_length = 20\n",
    "d_k = 64\n",
    "d_v = 64\n",
    "d_model = 512\n",
    "\n",
    "multi_head_attention = MultiHeadAttention(num_heads=num_heads, d_k=d_k, d_v=d_v, d_model=d_model)\n",
    "\n",
    "queries = torch.randn(batch_size, sequence_length, d_model)\n",
    "\n",
    "keys = torch.randn(batch_size, sequence_length, d_model)\n",
    "\n",
    "values = torch.randn(batch_size, sequence_length, d_model)\n",
    "\n",
    "print(f\"multi head attention output shape: {multi_head_attention(queries, keys, values).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/feed_forward.png\" height=\"250px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int = 2048):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(in_features=d_model, out_features=d_ff, bias=True)\n",
    "        self.linear_2 = nn.Linear(in_features=d_ff, out_features=d_model, bias=True)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of the position-wise feed forward network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input of shape (batch_size, sequence_length, d_model)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output of shape (batch_size, sequence_length, d_model)\n",
    "        \"\"\"\n",
    "        return self.linear_2(F.relu(self.linear_1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoke Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: torch.Size([2, 3, 512])\n",
      "position wise feed forward network output shape: torch.Size([2, 3, 512])\n"
     ]
    }
   ],
   "source": [
    "d_model = 512\n",
    "\n",
    "position_wise_feed_forward_network = PositionWiseFeedForwardNetwork(d_model=d_model, d_ff=2048)\n",
    "\n",
    "batch_size = 2\n",
    "sequence_length = 3\n",
    "\n",
    "x = torch.randn(batch_size, sequence_length, d_model)\n",
    "\n",
    "print(f\"x.shape: {x.shape}\")\n",
    "print(f\"position wise feed forward network output shape: {position_wise_feed_forward_network(x).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/transformer.png\" height=\"500px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, num_heads: int, d_k: int, d_v: int, d_model: int):\n",
    "        super().__init__()\n",
    "        self.multi_head_attention = MultiHeadAttention(num_heads=num_heads, d_k=d_k, d_v=d_v, d_model=d_model)\n",
    "        self.layer_normalization_1 = nn.LayerNorm(normalized_shape=d_model)\n",
    "        self.position_wise_feed_forward_network = PositionWiseFeedForwardNetwork(d_model=d_model, d_ff=2048)\n",
    "        self.layer_normalization_2 = nn.LayerNorm(normalized_shape=d_model)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of the encoder layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input of shape (batch_size, sequence_length, d_model)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output of shape (batch_size, sequence_length, d_model)\n",
    "        \"\"\"\n",
    "        x = x + self.multi_head_attention(x, x, x)\n",
    "        x = self.layer_normalization_1(x)\n",
    "        x = x + self.position_wise_feed_forward_network(x)\n",
    "        x = self.layer_normalization_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "class DecoderLayer(nn.Module):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokens_dictionary_size: int,\n",
    "        max_sequence_length: int,\n",
    "        num_heads: int,\n",
    "        sequence_length: int,\n",
    "        d_model: int = 512,\n",
    "        num_layers: int = 6\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=tokens_dictionary_size, embedding_dim=d_model)\n",
    "        self.positional_encoding = PositionalEncoding(max_seq_length=max_sequence_length, d_model=d_model)\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(num_heads=num_heads, d_k=sequence_length, d_v=sequence_length, d_model=d_model) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(h=num_heads, d_k=sequence_length, d_v=sequence_length, d_model=d_model) for _ in range(num_layers)])\n",
    "        self.output_linear = nn.Linear(in_features=d_model, out_features=tokens_dictionary_size)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of the transformer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input of shape (batch_size, sequence_length)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output of shape TODO\n",
    "        \"\"\"\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            x = encoder_layer(x)\n",
    "        # TODO : Decoder part\n",
    "        for decoder_layer in self.decoder_layers:\n",
    "            x = decoder_layer(x)\n",
    "        \n",
    "        x = self.output_linear(x)\n",
    "        \n",
    "        x = F.softmax(x, dim=-1)\n",
    "        \n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
